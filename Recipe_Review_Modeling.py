# -*- coding: utf-8 -*-
"""analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD_uq9j6cRyI8rPpn9DcGRj0QjhqKG-P

#Imports
"""

import pandas as pd

from collections import Counter, defaultdict

import re

import random

import string

import numpy as np

from bs4 import BeautifulSoup

import matplotlib.pyplot as plt

import nltk
nltk.download('punkt_tab')
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis


from sentence_transformers import SentenceTransformer

!pip install keybert
from keybert import KeyBERT

"""#Data Collection/Cleaning"""

url = "https://raw.githubusercontent.com/tinh0/recipe-sentiment-analysis/main/reviews.csv"
df = pd.read_csv(url)
#print(df.head(5))

# Fixed cleaning text a little
def clean_text(t):
  t = re.sub(r'<[^>]+>', ' ', t)
  t = t.lower()
  t = re.sub(r'[^a-z\s]', ' ', t)
  return t

df['clean_review'] = df['review_text'].apply(clean_text)

df.columns

"""distribution of ratings"""

rating_counts = df['starRating'].value_counts().sort_index().reset_index()
rating_counts.columns = ['starRating', 'count']

print(rating_counts)

# # Histogram Plot
# counts, bins, patches = plt.hist(
#     df['starRating'],
#     bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],
#     edgecolor='black'
# )
# # Add labels above bars
# for count, x in zip(counts, bins[:-1]):
#     plt.text(x + 0.5, count + 0.1, int(count), ha='center', va='bottom')

# plt.xticks([1, 2, 3, 4, 5])
# plt.grid(False)
# plt.title("Distribution of Star Ratings")
# plt.xlabel("Rating")
# plt.ylabel("Frequency")
# plt.show()

def count_words_in_group(texts):
    texts = [t for t in texts if t.strip()]  # remove empty strings
    if not texts:
        return Counter()
    return Counter(word.lower() for text in texts for word in text.split())

word_counts_by_recipe = df.groupby('docId')['clean_review'].apply(count_words_in_group)

"""#K-grams"""

def get_k_grams_ratings(k, min_occurrences):
  counts = Counter()
  rating_sum = defaultdict(float)
  for text, rating in zip(df['clean_review'], df['starRating']):
      toks = [t for t in word_tokenize(text.lower()) if t.isalnum()]
      grams = list(ngrams(toks, k))
      if not grams:
            continue

      for g in grams:
            counts[g] += 1
            rating_sum[g] += float(rating)

  rows = []
  for g, c in counts.items():
  # must have at least n occurrences
      if c > min_occurrences:
          rows.append((' '.join(g), c, rating_sum[g] / c))

  return rows[:]

three_word_gram = get_k_grams_ratings(k=3, min_occurrences=0)
three_word_gram.sort(key=lambda x: x[2])  # sorts by ratings
len(three_word_gram)

doc_to_kset = {}

def get_k_grams(k, min_occurrences):
  counts = Counter()
  for text, docId in zip(df['clean_review'], df['docId']):
      toks = [t for t in word_tokenize(text.lower()) if t.isalnum()]
      grams = list(ngrams(toks, k))
      if docId not in doc_to_kset:
            doc_to_kset[docId] = []
      if not grams:
            continue
      for g in grams:
            counts[g] += 1
            if g not in doc_to_kset[docId]:
              doc_to_kset[docId].append(g)
  rows = []
  for g, c in counts.items():
  # must have at least n occurrences
      if c > min_occurrences:
          rows.append(g)
  return rows[:]

kgrams = get_k_grams(k=3, min_occurrences=0)
# print(kgrams)
# print(doc_to_kset)
# print(docs)

vectorizer = CountVectorizer(
    analyzer='word',
    ngram_range=(3, 3),
    max_features=5000,
    min_df=5,
    max_df=0.8,
)

feature_matrix = vectorizer.fit_transform(df['clean_review'])

"""# TF-IDF & KeyBert"""

tfidf_3gram = TfidfVectorizer(
    analyzer='word',
    ngram_range=(3, 3),   # Only trigrams
    max_features=5000,    # Optional: cap vocabulary size
    min_df=3,             # Ignore rare phrases
    max_df=0.8            # Ignore very common ones
)

tfidf_result = tfidf_3gram.fit_transform(df['clean_review'])
# print('\nidf values:')
# for ele1, ele2 in zip(tfidf_result.get_feature_names_out(), tfidf_result.idf_):
#     print(ele1, ':', ele2)

# print('\nWord indexes:')
# print(tfidf_result.vocabulary_)
# print('\ntf-idf value:')
print(tfidf_result)
# print('\ntf-idf values in matrix form:')
# print(tfidf_result.toarray())

kw_model = KeyBERT()
keybert_vec = [
    kw_model.extract_keywords(
        doc,
        keyphrase_ngram_range=(3, 3),
        stop_words="english",
        use_mmr=True,
        diversity=0.6,
        top_n=10
    )
    for doc in df["clean_review"].tolist()
]

df["keyphrases"] = [[p for p, s in doc] for doc in keybert_vec]

mlb = MultiLabelBinarizer()
keybert_result = mlb.fit_transform(df["keyphrases"])
print(keybert_result)

texts = df["clean_review"].astype(str).fillna("").tolist()

model = SentenceTransformer("all-MiniLM-L6-v2")
doc_emb = model.encode(
    texts,
    batch_size=64,
    convert_to_numpy=True,
    normalize_embeddings=True,
    show_progress_bar=True
)
print(doc_emb)

"""# PCA"""

SVD = TruncatedSVD(n_components=2, random_state=42)

feature_matrix_pca = SVD.fit_transform(feature_matrix)

# Plot for visualization
plt.scatter(feature_matrix_pca[:, 0], feature_matrix_pca[:, 1], c=df['starRating'], cmap='viridis')
plt.title("3-Gram + PCA")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(label="Rating")
plt.show()

result_tfidf_pca = SVD.fit_transform(tfidf_result)

# Plot for visualization
plt.scatter(result_tfidf_pca[:, 0], result_tfidf_pca[:, 1], c=df['starRating'], cmap='viridis')
plt.title("TF–IDF 3-gram + PCA")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(label="Rating")
plt.show()

pca_keybert = PCA(n_components=2, random_state=42)
keybert_pca = pca_keybert.fit_transform(keybert_result)

# Plot for visualization
plt.scatter(keybert_pca[:, 0], keybert_pca[:, 1], c=df['starRating'], cmap='viridis')
plt.title("KeyBert + PCA")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.colorbar(label="Rating")
plt.show()

pca = PCA(n_components=200)
X_reduced = pca.fit_transform(feature_matrix)

y = df['starRating'].astype(int)

lda = LinearDiscriminantAnalysis()
X_lda = lda.fit_transform(X_reduced, y)

plt.figure(figsize=(8,6))
scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('2D Projection onto Top 2 LDA Components from 3-Grams')
plt.grid(True)
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

pca = PCA(n_components=200)
X_reduced = pca.fit_transform(keybert_result)

y = df['starRating'].astype(int)

lda = LinearDiscriminantAnalysis()
X_lda = lda.fit_transform(X_reduced, y)

plt.figure(figsize=(8,6))
scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('2D Projection onto Top 2 LDA Components from KeyBert')
plt.grid(True)
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

feature_names = np.array(vectorizer.get_feature_names_out())
X_lda_coords = X_lda  # just a shorter name

ratings = sorted(np.unique(y))
examples_per_rating = 3
top_k_words = 5   # how many features to print per point

for rating in ratings:
    mask = (y == rating)
    idxs = np.where(mask)[0]

    xs = X_lda_coords[idxs, 0]
    ys = X_lda_coords[idxs, 1]

    # centroid for this rating in LDA space
    cx, cy = xs.mean(), ys.mean()

    # pick points closest to the centroid
    dists = (xs - cx) ** 2 + (ys - cy) ** 2
    order = np.argsort(dists)
    chosen = idxs[order[:examples_per_rating]]

    print(f"\n=== Examples for rating {rating} ★ ===")
    for j in chosen:
        row = feature_matrix[j]

        # convert row to dense 1D array
        if hasattr(row, "toarray"):
            row = row.toarray().ravel()
        else:
            row = np.asarray(row).ravel()

        # top-k features by weight for this doc
        top_idx = np.argsort(row)[::-1][:top_k_words]
        top_idx = [i for i in top_idx if row[i] > 0]
        words = feature_names[top_idx]

        xj, yj = X_lda_coords[j, 0], X_lda_coords[j, 1]
        print(f"- df index {j}, LDA ({xj:.3f}, {yj:.3f})")
        print("  words:", ", ".join(words))

pca = PCA(n_components=200)
X_reduced = pca.fit_transform(tfidf_result.toarray())

y = df['starRating'].astype(int)

lda = LinearDiscriminantAnalysis()
X_lda = lda.fit_transform(X_reduced, y)

plt.figure(figsize=(8,6))
scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('2D Projection onto Top 2 LDA Components from TF-IDF')
plt.grid(True)
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

feature_names = np.array(vectorizer.get_feature_names_out())
X_lda_coords = X_lda  # just a shorter name

ratings = sorted(np.unique(y))
examples_per_rating = 3
top_k_words = 5   # how many features to print per point

for rating in ratings:
    mask = (y == rating)
    idxs = np.where(mask)[0]

    xs = X_lda_coords[idxs, 0]
    ys = X_lda_coords[idxs, 1]

    # centroid for this rating in LDA space
    cx, cy = xs.mean(), ys.mean()

    # pick points closest to the centroid
    dists = (xs - cx) ** 2 + (ys - cy) ** 2
    order = np.argsort(dists)
    chosen = idxs[order[:examples_per_rating]]

    print(f"\n=== Examples for rating {rating} ★ ===")
    for j in chosen:
        row = feature_matrix[j]

        # convert row to dense 1D array
        if hasattr(row, "toarray"):
            row = row.toarray().ravel()
        else:
            row = np.asarray(row).ravel()

        # top-k features by weight for this doc
        top_idx = np.argsort(row)[::-1][:top_k_words]
        top_idx = [i for i in top_idx if row[i] > 0]
        words = feature_names[top_idx]

        xj, yj = X_lda_coords[j, 0], X_lda_coords[j, 1]
        print(f"- df index {j}, LDA ({xj:.3f}, {yj:.3f})")
        print("  words:", ", ".join(words))

# print(feature_matrix.shape, tfidf_result.shape)


# similarity_count = cosine_similarity(feature_matrix)
# similarity_tfidf = cosine_similarity(tfidf_result)

!pip install umap-learn

"""# UMAP"""

import umap

reducer = umap.UMAP(n_neighbors=15, n_components=3, min_dist=0.1, random_state=42)

umap_result = reducer.fit_transform(doc_emb)

import plotly.express as px

umap_3d = pd.DataFrame(umap_result, columns=['x', 'y', 'z'])
umap_3d['rating'] = df['starRating'].values
umap_3d['review'] = df['review_text'].values

# plt.figure(figsize=(8,6))
# scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=df['starRating'], cmap='viridis', alpha=0.7)
# plt.xlabel('UMAP Component 1')
# plt.ylabel('UMAP Component 2')
# plt.title('UMAP on Sentence Embeddings')
# plt.grid(True)
# plt.legend(*scatter.legend_elements(), title="Rating")
# plt.show()

fig = px.scatter_3d(umap_3d, x='x', y='y', z='z', color='rating', hover_data=['review'], color_continuous_scale='viridis', opacity=0.6, title='3D UMAP Clustering')
fig.show()

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def run_tsne(X, ratings=None, title="t-SNE", pca_dim=50, tsne_dim=2):
    # PCA to reduce dimensionality first
    pca = PCA(n_components=pca_dim, random_state=42)
    X_pca = pca.fit_transform(X)

    # t-SNE on PCA output
    tsne = TSNE(n_components=tsne_dim, perplexity=30, random_state=42)
    X_tsne = tsne.fit_transform(X_pca)

    # Plot
    plt.figure(figsize=(8,6))
    if ratings is None:
        plt.scatter(X_tsne[:,0], X_tsne[:,1], alpha=0.6)
    else:
        plt.scatter(X_tsne[:,0], X_tsne[:,1], c=ratings, cmap='viridis', alpha=0.6)
        plt.colorbar(label="Rating")

    plt.title(title)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")
    plt.show()

    return X_tsne

tsne_tfidf = run_tsne(tfidf_result, ratings=df['starRating'], title="t-SNE: TF-IDF 3-gram")